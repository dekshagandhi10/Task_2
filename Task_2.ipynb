{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z61nmS3NjKEJ",
        "outputId": "b5aba935-70a9-4cc6-da02-cd99031f3b61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  5 07:28:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q diffusers transformers accelerate safetensors pillow\n"
      ],
      "metadata": {
        "id": "gGeS6wApk6eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Select model (you can change to any HF model like \"stabilityai/stable-diffusion-2-1\")\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Detect GPU/CPU\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# Load Stable Diffusion pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        ")\n",
        "pipe = pipe.to(DEVICE)\n"
      ],
      "metadata": {
        "id": "GBZz9n4klD7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A serene landscape of mountains reflected in a crystal-clear lake, cinematic lighting, ultra-realistic, 8K resolution\"\n",
        "\n",
        "# Optional: add negative prompt\n",
        "negative_prompt = \"low quality, blurry, watermark, text, distorted\"\n",
        "\n",
        "# For reproducibility\n",
        "seed = 123\n",
        "generator = torch.Generator(device=DEVICE).manual_seed(seed)\n",
        "\n",
        "# Generate\n",
        "image = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=30,\n",
        "    generator=generator\n",
        ").images[0]\n",
        "\n",
        "# Save and show\n",
        "output_path = f\"outputs/generated_image.png\"\n",
        "image.save(output_path)\n",
        "image.show()\n",
        "print(\"âœ… Image saved to:\", output_path)\n"
      ],
      "metadata": {
        "id": "DXA2e64YmE_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"A futuristic city skyline at sunset, neon reflections, ultra-detailed\",\n",
        "    \"A cozy coffee shop interior with vintage decor, warm lighting, 4K\",\n",
        "    \"An astronaut riding a horse on Mars, cinematic composition\"\n",
        "]\n",
        "\n",
        "for i, p in enumerate(prompts):\n",
        "    generator = torch.Generator(device=DEVICE).manual_seed(100 + i)\n",
        "    img = pipe(p, guidance_scale=7.5, num_inference_steps=30, generator=generator).images[0]\n",
        "    img.save(f\"outputs/prompt_{i+1}.png\")\n",
        "    print(f\"Saved outputs/prompt_{i+1}.png\")\n"
      ],
      "metadata": {
        "id": "oFkzYL8cmxy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}